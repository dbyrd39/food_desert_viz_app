{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09dcb388",
   "metadata": {},
   "source": [
    "# INGEST.ipynb \n",
    "# Data Retrieval Tools\n",
    "\n",
    "---\n",
    "## Overview\n",
    "- fetch_census_tracts.py\n",
    "- fetch_nc_counties.py\n",
    "- fetch_osm_outlets.py\n",
    "- fetch_population.py\n",
    "- fetch_usda_food_access.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c8d25",
   "metadata": {},
   "source": [
    "# 1 - fetch_census_tracts.py\n",
    "\n",
    "This file retrieves North Carolina census tract data as a GeoPandas GeoDataFrame. \n",
    "\n",
    "NOTE: This project deals with some pretty heavy geographic data analysis, so here is a little bit of design background for a better understanding of how census tract data is constructed and how it is used in this application: \n",
    "\n",
    "Census tracts are small geographic units consisting of 1,200-8,000 people. They are designed for statistical analysis of localized areas, allowing for a deeper view than county or city-level analysis. The narrow size and permanent boundaries allow for an easy comparison across decades, and they can be used to study socioeconomic disparities. This project uses Cartographic Boundaries, which have simplified geometries comparied to the US Census Bureau's typical TIGER/Line files and excel in visualization over precision. The project is already heavy, so optimizing for speed allows the application to run effectively at a state-wide level. \n",
    "\n",
    "This file makes use of caching. Repeatedly retrieving remote data increases runtime and runs the risk of encountering network issues, so zip files are cached to avoid re-downloading with each run and avoiding network issues downstream. \n",
    "\n",
    "The zip file used in this analysis is a shapefile, which is the GIS standard for storing geographic features and their attributes. The contents of an shapefile include a .shp file (feature geometry), .shx file (geometry index), .dbf file (dBASE table containing feature attributes), and an optional .prj file (defines coordinate system). \n",
    "\n",
    "Shapefiles usually contain CRS metadata. CRS (Coordinate Reference System) brings context to coordinate data, explaining where coordinates are located, what units they use, and how they measure direction and angles. EPSG is a standardized registry of coordinate systems. Most US government census data uses EPSG:4269 (NAD83). However, this application uses EPSG:4326 instead, as do most web maps. An earlier version of this project used 4269, but the layers were incorrectly offset from their intended location. \n",
    "\n",
    "GEOIDs are hierarchical geographic identifiers. They are used across multiple datasets, including census datasets. They are the canonical join-keys for census geography. However, some shapefiles omit GEOIDs. Thus, this file generates one if it does not exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085fa05b",
   "metadata": {},
   "source": [
    "# 1.1 - Imports and Globals\n",
    "\n",
    "This section covers necessary imports and global variables.\n",
    "\n",
    "Annotations/type hints are incorporated for clarity. Path is imported to work with file systems. Zipfile is used for reading/extracting `.zip` files. Requests is used for remote data retrieval. GeoPandas is used for geospatial data management. Additionally, a useful function `ensure_dir` from the `utils.cache` module is imported. \n",
    "\n",
    "`TIGER_TRACT_ZIP_URL` is a global variable that points to the Census TIGER/Cartographic Boundary shapefile ZIP for North Carolina census tracts in 2023, with a resolution of 500,000 generalized boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a814fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "\n",
    "from src.utils.cache import ensure_dir\n",
    "\n",
    "TIGER_TRACT_ZIP_URL = \"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_37_tract_500k.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b6b5f2",
   "metadata": {},
   "source": [
    "# 1.2 - Download NC Tracts Zip\n",
    "\n",
    "The function `download_nc_tracts_zip` simply downloads the census tract data from a cached directory or URL. The function accepts a Path `cache_dir`, and a string `url` that defaults to `TIGER_TRACT_ZIP_URL`. In practice, `cache_dir` and `url` are sources for accessing the census tract data. For speed and memory purposes, the function first attempts to retrieve data from `cache_dir` first, then resorts to `url` as a fallback if the first attempt fails. The function returns a Path, which is the path to the downloaded zip file.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Before proceeding, ensures that `cache_dir` exists.\n",
    "- Construct the output filename inside the cache director and store it as `out`.\n",
    "- If the zip file exists locally, skip downloading and return the cached file path.\n",
    "- Otherwise, send an HTTP Get request. Set `stream` to True to read the data in chunks, and set the request to timeout after hanging for 120 seconds.\n",
    "- If HTTP request is not 200-ish (likely 404 or 500), then raise an exception immediately.\n",
    "- Open output file `out` in write-binary mode.\n",
    "- Iterate through the response body in 1 MB chunks\n",
    "- Guard against keep-alive chunks, which are rare but possible.\n",
    "- Write each chunk to disc.\n",
    "- Return the Path to the downloaded zip file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1537dba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def download_nc_tracts_zip(cache_dir: Path, url: str = TIGER_TRACT_ZIP_URL) -> Path:\n",
    "    ensure_dir(cache_dir)\n",
    "    out = cache_dir / \"cb_2023_37_tract_500k.zip\"\n",
    "    if out.exists():\n",
    "        return out\n",
    "    r = requests.get(url, stream=True, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    with out.open(\"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0e33d",
   "metadata": {},
   "source": [
    "# 1.3 - Extract Zip\n",
    "\n",
    "This function extracts a zip file into a directory and returns the extraction directory path. The function accepts a Path `zip_path` and a Path `extract_dir`, and it returns a Path. In practice, `zip_path` is the location of the zip file to extract, `extract_dir` is the directory to extract into, and the return object is `extract_dir` which now contains the zip file contents.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Ensure the extraction directory exists.\n",
    "- Open the zip archive for reading.\n",
    "- Extract all files from the zip file into `extract_dir`.\n",
    "- Return the directory where files were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6bf46d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_zip(zip_path: Path, extract_dir: Path) -> Path:\n",
    "    ensure_dir(extract_dir)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(extract_dir)\n",
    "    return extract_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5b5a2",
   "metadata": {},
   "source": [
    "# 1.4 - Load NC Tracts GDF\n",
    "\n",
    "This function downloads the zip if needed, extracts it, loads the tract shapefile into a GeoPandas dataframe, normalizes CRS, ensures a GEOID column exists, and returns a GeoDataFrame. The function accepts a Path `cache_dir` for use in the `download_nc_tracts_zip` function.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Download the zip (or use cached) and store its path.\n",
    "- Extract the zip contents into a subfolder inside `cache_dir`.\n",
    "- Find shapefiles (.shp) in the extracted directory.\n",
    "- If no shapefile exists, raise a clear error statement.\n",
    "- Read the first shapefile found into a GeoDataFrame\n",
    "- If the shapefile lacks CRS metadata, then assign CRS EPSG:4269 (NAD83).\n",
    "- Reproject to EPSG:4326 (WGS84 lat/lon) — standard for web maps.\n",
    "- If a tract identifier column does not exist, theb build a list of the component columns needed to construct GEOID (only include those that exist).\n",
    "- If all three components exist, then construct GEOID by concatenating state FIPS, county FIPS, and tract code (all cast to strings to avoid numeric concatenation issues).\n",
    "- If GEOID is missing and can't be built, raise a clear error statement.\n",
    "- Return the resulting GeoPandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6dd44f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_nc_tracts_gdf(cache_dir: Path) -> gpd.GeoDataFrame:\n",
    "    zip_path = download_nc_tracts_zip(cache_dir)\n",
    "    shp_dir = extract_zip(zip_path, cache_dir / \"cb_2023_37_tract_500k\")\n",
    "    shp_files = list(shp_dir.glob(\"*.shp\"))\n",
    "    if not shp_files:\n",
    "        raise FileNotFoundError(f\"No .shp found in {shp_dir}\")\n",
    "    gdf = gpd.read_file(shp_files[0])\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(\"EPSG:4269\", allow_override=True)\n",
    "    gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    if \"GEOID\" not in gdf.columns:\n",
    "        cols = [c for c in [\"STATEFP\", \"COUNTYFP\", \"TRACTCE\"] if c in gdf.columns]\n",
    "        if len(cols) == 3:\n",
    "            gdf[\"GEOID\"] = (\n",
    "                gdf[\"STATEFP\"].astype(str)\n",
    "                + gdf[\"COUNTYFP\"].astype(str)\n",
    "                + gdf[\"TRACTCE\"].astype(str)\n",
    "            )\n",
    "        else:\n",
    "            raise KeyError(\"GEOID not found and cannot be constructed from STATEFP/COUNTYFP/TRACTCE.\")\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0a5c6",
   "metadata": {},
   "source": [
    "# 2 - fetch_nc_counties.py\n",
    "\n",
    "This file is responsible for retrieving North Carolina county data. The retrieved data is a GeoDataFrame containing county names and the associated geometry polygons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44486ace",
   "metadata": {},
   "source": [
    "# 2.1 - Imports and Globals\n",
    "\n",
    "This section covers necessary imports and global variables.\n",
    "\n",
    "Path is imported to work with file systems. GeoPandas is used for geospatial data management. Requests is used for remote data retrieval. Zipfile is used for reading/extracting `.zip` files. Additionally, a useful function `ensure_dir` from the `utils.cache` module is imported. \n",
    "\n",
    "`TIGER_COUNTY_URL` is a global variable that points to the Census TIGER/Line shapefile ZIP for all US counties in 2022, which will be filtered to only NC later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d57db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "from src.utils.cache import ensure_dir\n",
    "\n",
    "TIGER_COUNTY_URL = (\n",
    "    \"https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad083cc",
   "metadata": {},
   "source": [
    "# 2.2 - Load NC Counties\n",
    "\n",
    "This function is responsible for loading North Carolina counties as a GeoPandas GeoDataFrame and returning it. The function accepts a Path `cache_dir`, which in practice is a cached directory that holds the county data for quick and easy retrieval. \n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Make sure `cache_dir` exists before saving zip file or extracted files.\n",
    "- The path name of the downloaded zip file in the cached diectory is stored as `zip_path`.\n",
    "- If the zip file is located in the cache, then skip the downloads. Otherwise, download county data with requests, throw an error if it fails, and write to output file (binary writing, 1 MB chunks).\n",
    "- Create `extract_dir`, the folder to store extract files.\n",
    "- If already extracted, skip extraction.\n",
    "- Open zip file and extract contents into `extract_dir`.\n",
    "- Find all shapefiles in the extraction directory, and throw error if no shapefiles exist.\n",
    "- Load shapefile as a GeoDataFrame and convert to EPSG:4326.\n",
    "- Filter GeoDataFrame using the FIPS code \"37\" (a string, not an integer).\n",
    "- Return only the county name and the geometry polygon (everything else is not needed for this project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7674fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_nc_counties(cache_dir: Path) -> gpd.GeoDataFrame:\n",
    "    ensure_dir(cache_dir)\n",
    "\n",
    "    zip_path = cache_dir / \"tl_2022_us_county.zip\"\n",
    "\n",
    "    # Download once\n",
    "    if not zip_path.exists():\n",
    "        print(\"   downloading TIGER/Line US counties…\")\n",
    "        r = requests.get(TIGER_COUNTY_URL, stream=True, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        with zip_path.open(\"wb\") as f:\n",
    "            for chunk in r.iter_content(1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    extract_dir = cache_dir / \"tl_2022_us_county\"\n",
    "    if not extract_dir.exists():\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            z.extractall(extract_dir)\n",
    "\n",
    "    shp_files = list(extract_dir.glob(\"*.shp\"))\n",
    "    if not shp_files:\n",
    "        raise FileNotFoundError(\"County shapefile not found after extraction.\")\n",
    "\n",
    "    gdf = gpd.read_file(shp_files[0]).to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Filter to North Carolina (STATEFP = '37')\n",
    "    gdf = gdf[gdf[\"STATEFP\"] == \"37\"]\n",
    "\n",
    "    # Keep only what we need\n",
    "    return gdf[[\"NAME\", \"geometry\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506cd9e",
   "metadata": {},
   "source": [
    "# 3 - fetch_osm_outlets.py\n",
    "\n",
    "This file is responsible for accessing OpenStreetMap and retrieving food location data, the key piece to determining food access conditions. Additionally, the file retrieves healthy and unhealthy outlets for nutritional comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23521d28",
   "metadata": {},
   "source": [
    "# 3.1 - Imports and Globals\n",
    "\n",
    "This section is responsible for necessary imports and globals.\n",
    "\n",
    "Annotations/type hints are included for clarity. Path is used for file system management. Json, dataclass, and Pandas are imported for data management. Requests is used for data retrieval, and time is used for separating queries. \n",
    "\n",
    "`OVERPASS_URLS` contains three urls corresponding to different Overpass servers. Since Overpass frequently rate-limits, having multiple options is robust.\n",
    "\n",
    "BoundingBox is an object holding south latitude, west longitude, north latitude, and east longitude. The default bounding box is the North Carolina region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259408e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "OVERPASS_URLS = [\n",
    "    \"https://overpass-api.de/api/interpreter\",\n",
    "    \"https://overpass.kumi.systems/api/interpreter\",\n",
    "    \"https://overpass.nchc.org.tw/api/interpreter\",\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BoundingBox:\n",
    "    south: float\n",
    "    west: float\n",
    "    north: float\n",
    "    east: float\n",
    "\n",
    "NC_BBOX = BoundingBox(south=33.75, west=-84.45, north=36.6, east=-75.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f97a871",
   "metadata": {},
   "source": [
    "# 3.2 - Overpass Query\n",
    "\n",
    "This function wraps a query body inside a full Overpass QL query. The function accepts a BoundingBox `bbox` (region) and a string `q_body` (query body). The function returns a string, which in this case is an Overpass QL query.\n",
    "\n",
    "NOTE: Both ways and nodes are fetched in this file. `out center` ensures that Overpass returns a center coordinate for ways (polygons) and lat/lon for nodes (points). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155f795",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def _overpass_query(bbox: BoundingBox, q_body: str) -> str:\n",
    "    return f\"\"\"[out:json][timeout:180];\n",
    "(\n",
    "{q_body}\n",
    ");\n",
    "out center;\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e218dd9",
   "metadata": {},
   "source": [
    "# 3.3 - Fetch Overpass\n",
    "\n",
    "This function actually fetches data from Overpass using a string `query`, and it returns a dictionary representing a JSON response. \n",
    "\n",
    "Line-by-line breakdown:\n",
    "- `last_err` will be used to store errors for reporting.\n",
    "- Loop through each url, trying a request with a 5 minute client-side timeout\n",
    "- Throw an error for HTTP 400ish/500ish errors\n",
    "- Return the parsed JSON response\n",
    "- If Exception occurs, then store error.\n",
    "- Since a success would have led to an early return, the end of this function raises a RuntimeError to indicate response failure along with the exact error `last_err`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6fc3c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def _fetch_overpass(query: str) -> dict:\n",
    "    last_err = None\n",
    "    for url in OVERPASS_URLS:\n",
    "        try:\n",
    "            r = requests.post(url, data={\"data\": query}, timeout=300)\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(5)\n",
    "    raise RuntimeError(f\"All Overpass endpoints failed. Last error: {last_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47544ec2",
   "metadata": {},
   "source": [
    "# 3.3 - Fetch With Cache\n",
    "\n",
    "This function is responsible for fetching data locally. `CACHE_DIR` is the cache directory location. The function accepts a string `cache_name` and a string `query`, and it returns a dictionary (JSON data).\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Ensure cache directory exists. \n",
    "- If cached response exists, load JSON and return.\n",
    "- Otherwise, fetch data from overpass using the `query` parameter.\n",
    "- Write JSON to disk, and return data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7c584",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "CACHE_DIR = Path(__file__).resolve().parents[3] / \"data\" / \"raw\" / \"cache\"\n",
    "\n",
    "def _fetch_with_cache(cache_name: str, query: str) -> dict:\n",
    "    CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    path = CACHE_DIR / cache_name\n",
    "    if path.exists():\n",
    "        return json.loads(path.read_text())\n",
    "    data = _fetch_overpass(query)\n",
    "    path.write_text(json.dumps(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40020977",
   "metadata": {},
   "source": [
    "# 3.4 - Elements To Points\n",
    "\n",
    "This function takes Overpass elements (nodes and ways), and returns a dataframe of normalized point records. The parameters are a list of dictionaries `elements` and a string `outlet_type`.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Build a list to store row dictionaries.\n",
    "- Iterate through each element.\n",
    "- Overpass elements store metadate in a `tags` dictionary.\n",
    "- Derive a human-readable name, using \"Unknown\" as a fallback.\n",
    "- Handle both node and way geometry types, skipping elements without coordinates.\n",
    "- Create a row consisting of a name/label, latitude, longitude, outlet type (healthy or unhealthy) and a `osm_id`, and append to `rows`.\n",
    "- After the loop exits, return the row dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a037a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def _elements_to_points(elements: list[dict], outlet_type: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for el in elements:\n",
    "        tags = el.get(\"tags\", {})\n",
    "        name = tags.get(\"name\") or tags.get(\"brand\") or tags.get(\"operator\") or \"Unknown\"\n",
    "        lat = el.get(\"lat\") or (el.get(\"center\") or {}).get(\"lat\")\n",
    "        lon = el.get(\"lon\") or (el.get(\"center\") or {}).get(\"lon\")\n",
    "        if lat is None or lon is None:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"name\": name,\n",
    "            \"lat\": float(lat),\n",
    "            \"lon\": float(lon),\n",
    "            \"outlet_type\": outlet_type,\n",
    "            \"osm_id\": f\"{el.get('type','')}/{el.get('id','')}\",\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12972b9a",
   "metadata": {},
   "source": [
    "# 3.5 - Fetch Healthy Outlets\n",
    "\n",
    "This function retrieves healthy food outlets from OSM. The function accepts a BoundingBox `bbox` (defaulting to NC region) and returns a Pandas dataframe consisting of healthy food outlet point data.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Extract bounding box coordinates.\n",
    "- Build query body: pulling point and polygon features for supermarkets, grocery stoers, and markets. \n",
    "- Wrap query body into a full Overpass query, using `_fetch_with_cache` to use cached JSON if it exists.\n",
    "- Convert Raw OSM elements into points and return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945fe8df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_healthy_outlets(bbox: BoundingBox = NC_BBOX) -> pd.DataFrame:\n",
    "    s,w,n,e = bbox.south, bbox.west, bbox.north, bbox.east\n",
    "    q_body = f\"\"\"\n",
    "node[shop=supermarket]({s},{w},{n},{e});\n",
    "way[shop=supermarket]({s},{w},{n},{e});\n",
    "node[shop=grocery]({s},{w},{n},{e});\n",
    "way[shop=grocery]({s},{w},{n},{e});\n",
    "node[amenity=marketplace]({s},{w},{n},{e});\n",
    "way[amenity=marketplace]({s},{w},{n},{e});\n",
    "\"\"\"\n",
    "    data = _fetch_with_cache(\"osm_healthy.json\", _overpass_query(bbox, q_body))\n",
    "    return _elements_to_points(data.get(\"elements\", []), \"healthy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb725df",
   "metadata": {},
   "source": [
    "# 3.6 - Fetch Unhealthy Outlets\n",
    "\n",
    "This function retrieves unhealthy food outlets from OSM. The function accepts a BoundingBox `bbox` (defaulting to NC region) and returns a Pandas dataframe consisting of unhealthy food outlet point data.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Extract bounding box coordinates.\n",
    "- Build query body: pulling point and polygon features for fast food locations\n",
    "- Wrap query body into a full Overpass query, using `_fetch_with_cache` to use cached JSON if it exists.\n",
    "- Convert Raw OSM elements into points and return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c9f117",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_unhealthy_outlets(bbox: BoundingBox = NC_BBOX) -> pd.DataFrame:\n",
    "    s,w,n,e = bbox.south, bbox.west, bbox.north, bbox.east\n",
    "    q_body = f\"\"\"\n",
    "node[amenity=fast_food]({s},{w},{n},{e});\n",
    "way[amenity=fast_food]({s},{w},{n},{e});\n",
    "\"\"\"\n",
    "    data = _fetch_with_cache(\"osm_unhealthy.json\", _overpass_query(bbox, q_body))\n",
    "    return _elements_to_points(data.get(\"elements\", []), \"unhealthy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493108a1",
   "metadata": {},
   "source": [
    "# 4 - fetch_population.py\n",
    "\n",
    "This file is responsible for retrieving population data for each census tract. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b839f7",
   "metadata": {},
   "source": [
    "# 4.1 - Imports and Globals\n",
    "\n",
    "This section covers necessary imports and global variables.\n",
    "\n",
    "Annotations/type hints are incorporated for clarity. Path is imported to work with file systems. Requests is used for remote data retrieval. Pandas is used for data management. \n",
    "\n",
    "`ACS_URL` is a global variable that points to the ACS 5-year estimates population survey from 2022. This project specifically utilizes ACS 5-year because it covers all census tracts, it is more statistically stable than a 1-year estimate, and it is the standard choice for tract-level analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb73ae3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "ACS_URL = \"https://api.census.gov/data/2022/acs/acs5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273480c",
   "metadata": {},
   "source": [
    "# 4.2 - Fetch NC Tract Population\n",
    "\n",
    "This function retrieves the total population for each census tract in North Carolina based on the ACS 5-year estimates. The function returns a Pandas dataframe containing a GEOID and the corresponding population.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Initialize `params`, a dictionary containing a Census API query. \"B01003_001E\" is a total population table estimate, \"tract:*\" requests all tracts, and \"state:37\" refers to North Carolina. \n",
    "- Make the request, and timeout after 60 seconds of inactivity. Throw an error statement if it fails.\n",
    "- Store the JSON object as `data`.\n",
    "- Convert data into a dataframe `df`.\n",
    "- Generate a GEOID manually.\n",
    "- Clean population values and ensure they are integers.\n",
    "- Return the dateframe, including only GEOID and associated population values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcad9c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_nc_tract_population() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch total population per census tract in North Carolina\n",
    "    using ACS 5-year estimates.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"get\": \"B01003_001E\",\n",
    "        \"for\": \"tract:*\",\n",
    "        \"in\": \"state:37\",\n",
    "    }\n",
    "\n",
    "    r = requests.get(ACS_URL, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    df = pd.DataFrame(data[1:], columns=data[0])\n",
    "    df[\"GEOID\"] = (\n",
    "        df[\"state\"]\n",
    "        + df[\"county\"]\n",
    "        + df[\"tract\"]\n",
    "    )\n",
    "\n",
    "    df[\"population\"] = pd.to_numeric(df[\"B01003_001E\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    return df[[\"GEOID\", \"population\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e263b",
   "metadata": {},
   "source": [
    "# 5 - fetch_usda_food_access.py\n",
    "\n",
    "This file is responsible for retrieving and loading USDA food access data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23d243",
   "metadata": {},
   "source": [
    "# 5.1 - Imports and Globals\n",
    "\n",
    "This section covers necessary imports and global variables.\n",
    "\n",
    "Annotations/type hints are incorporated for clarity. Path is imported to work with file systems. Pandas is used for data management. Requests is used for remote data retrieval. Additionally, a useful function `ensure_dir` from the `utils.cache` module is imported. \n",
    "\n",
    "`DEFAULT_LOCAL_NAME` is a global variable that points to a local .csv file containing food access data from the USDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c7ee8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "from src.utils.cache import ensure_dir\n",
    "\n",
    "DEFAULT_LOCAL_NAME = \"usda_food_access.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038713a8",
   "metadata": {},
   "source": [
    "# 5.2 - Get USDA Food Access\n",
    "\n",
    "This function ensures that the USDA Food Access CSV exists by finding it or writing it to disk. The function accepts a Path `cache_dir` (directory on desk to store file), and a string `url` (url to retrieve data from). The function returns a Path (local path to file).\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Guarantee a cache directory exists before writing.\n",
    "- Construct the full path to the expected CSV file.\n",
    "- Early return if file exists already.\n",
    "- If file does not exist and no URL is provided, then throw a clear error.\n",
    "- Otherwise, make request.\n",
    "- Throw error if status is 400ish/500ish.\n",
    "- Open destination file in binary write mode.\n",
    "- Iterate through 1 MB chunks, writing only non-empty chunks.\n",
    "- Return local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc7b12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_usda_food_access(cache_dir: Path, url: str | None = None) -> Path:\n",
    "    ensure_dir(cache_dir)\n",
    "    out = cache_dir / DEFAULT_LOCAL_NAME\n",
    "    if out.exists():\n",
    "        return out\n",
    "    if not url:\n",
    "        raise FileNotFoundError(\n",
    "            f\"USDA food access CSV not found at {out}. \"\n",
    "            \"Place it there or pass a download URL via url=...\"\n",
    "        )\n",
    "    r = requests.get(url, stream=True, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    with out.open(\"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e412121",
   "metadata": {},
   "source": [
    "# 5.3 - Load USDA Food Access\n",
    "\n",
    "This function loads the USDA csv from the disk. The function accepts a Path `cache_dir` (location of file on disk) and a string `url` (url to download data from), and it returns a Pandas dataframe (csv file of data, converted into a dataframe). This function simply wraps the get function above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d8783",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_usda_food_access(cache_dir: Path, url: str | None = None) -> pd.DataFrame:\n",
    "    return pd.read_csv(get_usda_food_access(cache_dir, url=url), low_memory=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
